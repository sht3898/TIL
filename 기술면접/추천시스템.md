# 추천 시스템

[참고링크1](https://lsjsj92.tistory.com/563)

[참고링크2][https://greeksharifa.github.io/machine_learning/2019/12/17/Recommendation-System/#2-%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%98-%EA%B0%9C%EC%9A%94](https://greeksharifa.github.io/machine_learning/2019/12/17/Recommendation-System/#2-추천-시스템의-개요)

과거에는 콘텐츠 기반 필터링과 최근접 이웃 협업 필터링이 주목 받았지만, 2009년에 있었던 넷플릭스 추천 컴퍼티션에서 행렬 분해를 이용한 잠재 요인 협업 필터링이 우승을 차지하면서 상황이 달라짐

![img](https://greeksharifa.github.io/public/img/Machine_Learning/2019-12-17-Recommendation%20System/01.JPG)



## 콘텐츠 기반 필터링

어떤 사용자가 특정 아이템을 선호할 때, 그 아이템과 비슷한 컨텐츠를 가진 다른 아이템을 추천하는 것이 이 방식의 기본 아이디어이다. 추가적으로 설명하자면, 이 방식은 사용자와 아이템에 대한 프로필을 만들고 그 특징을 활용한다. 예를 들어 어떤 특정 영화는 장르, 출연배우, 박스오피스 인기도 등 여러 특성을 지니게 될 텐데 이 **특성**(**컨텐츠**)들이 이 영화의 프로필을 형성하는 것



## 협업 필터링

메모리 기반: 이미 있는 데이터((메모리)를 기반으로 필터링

모델 기반: 기계학습을 적용한 필터링 방식, 더 유용하다고 평가됨

여기서는 메모리 기반의 필터링만 정리

### 최근접 이웃

> 콜드스타트 문제 발생
>
> 공간 비효율
>
> 일반적으로 사용자 기반보다 아이템 기반의 효율이 더 좋음

모든 협업 필터링은 사용자-아이템 행렬 데이터에 의존. 사용자가 남긴 평점(rating) 데이터를 기반하여 남기지 않은 데이터를 추론하는 형식

![img](https://greeksharifa.github.io/public/img/Machine_Learning/2019-12-17-Recommendation%20System/02.JPG)

#### 사용자 기반 최근접 이웃

특정 사용자와 유사한 사용자들을 선정하고, 이들을 TOP-N이라고 명명한 뒤 이들이 선호하는 아이템을 특정 사용자에게 추천하는 방식



#### 아이템 기반 최근접 이웃

어떤 사용자가 A라는 아이템을 선호한다고 할 때, 그 사용자는 A와 유사한 B라는 아이템 역시 선호할 것이라는 가정 하에 추천을 진행하는 방식. 아이템 기반 방식이 사용자 기반 방식 보다 정확도가 높은 것이 일반적이기에 본 방식이 더욱 자주 사용



### 잠재 요인 협업 필터링

> 공간 절약의 장점

사용자-아이템 평점 행렬에 잠재되어 있는 어떤 요인(factor)이 있다고 가정하고, 행렬 분해를 통해 그 요인들을 찾아내는 방식이다. 이 **잠재 요인**은 구체적으로 정의하는 것이 때로는 어렵지만, 실제 시스템에서는 추천의 근거를 마련하는 데에 있어 큰 역할을 수행한다.

예를 들어보면, 영화 장르를 **잠재 요인**으로 설정할 수 있다. 어떤 사용자는 판타지 영화를 다른 어떤 영화보다 좋아한다고 하면, 이 사용자에게 있어 영화를 선택할 때 가장 중요한 기준(요인)은 판타지 영화이냐 아니냐가 될 가능성이 높다. 그리고 이 사용자에게 다른 영화를 추천해준다고 한다면, 판타지 영화를 추천하는 것이 가장 합리적일 가능성이 높다는 것이다. `잠재 요인 협업 필터링`은 이러한 **요인**들을 찾아 추천에 활용하게 된다.

지금부터는 이 **행렬 분해**를 어떻게 진행하는지에 대해 알아보도록 하겠다.



#### Singular Value Decomposition

`특이값 분해`는 **Spectral Decomposition**의 일반화 버전이라고 생각하면 쉽다. 즉, 정방행렬이라는 조건을 만족하지 않아도(행과 열의 개수가 달라도) 다차원 행렬을 저차원 행렬로 분해하는 차원 축소 기법이다.

**Spectral Decomposition**에 따르면 정방행렬 A는 아래와 같이 표현할 수 있다.

A=PΛP′=PΛPT=p∑i=1λieie′iA=PΛP′=PΛPT=∑i=1pλieiei′

여기서 PP는 λλ에 대응하는 고유벡터들을 열벡터로 가지는 **직교행렬**이다. ΛΛ는 AA의 고유값들을 대각원소로 가지는 **대각행렬**이다.

(m, n), m>n인 직사각 행렬 AA에 대해 `특이값 분해`를 실시하면 아래와 같이 표현될 수 있다.

A=UΣVTA=UΣVT

- UU: (m, m), AA의 left singular 벡터로 구성된 직교행렬
- VV: (n, n), AA의 right singular 벡터로 구성된 직교행렬
- ΣΣ: (m, n), 주 대각성분이 √λiλi인 직사각 대각행렬

AATAAT를 위 식으로 표현하면 아래와 같다.

AAT=UΣVTVΣUT=U(ΣΣT)UTAAT=UΣVTVΣUT=U(ΣΣT)UT

여기서 ΣΣTΣΣT는 ΛΛ이다. (직접 계산해보라) 이 때문에 결과적으로 식은 아래와 같이 정리된다.

AAT=UΛUTAAT=UΛUT

여기서 UU는 **정방행렬**이기에 위에서 본 **Spectral Decomposition**의 식을 참조하면, UU는 AATAAT를 **Eigenvalue Decomposition**으로 직교대각화하여 얻은 **직교행렬**임을 알 수 있다. AA의 rank가 k일 때, 이 UU의 왼쪽에서부터 k번째 열벡터까지를 **좌특이벡터**(Left Singular Vectors)라고 부른다.

같은 방식으로 ATA=VΛVTATA=VΛVT에서 VV는 ATAATA를 **Eigenvalue Decomposition**으로 직교대각화하여 얻은 **직교행렬**이 된다.

SVD를 기하학적으로 설명하면, VT,UVT,U에 의해서 A 행렬의 방향이 변화하게 되고 ΣΣ에 의해서 scale이 조정된다고 볼 수 있다.



#### Matrix Factorization

위에서 설명한 SVD는 잠재요인을 밝혀내기에 아주 적합한 방법이지만, 실제 현실에서 원행렬 A에는 결측값이(당연히 모든 사용자가 모든 아이템에 대해 평점을 남겼다면, 굳이 추천 시스템이 필요하지 않을 것이다.) 많다. 따라서 이를 대체할 근사적인 방법이 필요하며, 그 방법에는 `SGD(Stochastic Gradient Descent)` 또는 `ALS(Alternating Least Squares)`가 있다. 이 방법들에 대해서는 [다음 글](https://greeksharifa.github.io/machine_learning/2019/12/20/Matrix-Factorization/)을 참조하기 바란다.

`SGD`를 이용해서 행렬을 분해하면 다음과 같다.

![img](https://greeksharifa.github.io/public/img/Machine_Learning/2019-12-17-Recommendation%20System/03.JPG)

이 때 요인의 개수는 하이퍼파라미터로 임의로 조정하거나, Cross-Validation을 통해 최적의 값을 찾을 수 있다. 위에서 분해된 행렬을 다시 내적하여 원 행렬을 예측해보면 아래와 같이 크게 차이가 나지 않음을 알 수 있다.

![img](https://greeksharifa.github.io/public/img/Machine_Learning/2019-12-17-Recommendation%20System/04.JPG)
